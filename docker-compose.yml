version: "3.9"
services:
  db:
    image: postgres:15
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: agentic
    ports:
      - "5432:5432"
    volumes:
      - db-data:/var/lib/postgresql/data
  redis:
    image: redis:7
    ports:
      - "16379:6379"
  api:
    build:
      context: .
      dockerfile: services/api/Dockerfile
    environment:
      DATABASE_URL: postgresql://postgres:postgres@db:5432/agentic
      REDIS_URL: redis://redis:6379/0
      LLM_PROVIDER: ${LLM_PROVIDER:-mock}
      OPENAI_MODEL: ${OPENAI_MODEL:-}
      OTEL_EXPORTER_OTLP_ENDPOINT: http://jaeger:4318
      POLICY_GATE_ENABLED: ${POLICY_GATE_ENABLED:-false}
      ORCHESTRATOR_ENABLED: ${ORCHESTRATOR_ENABLED:-true}
      ORCHESTRATOR_RECOVER_PENDING: ${ORCHESTRATOR_RECOVER_PENDING:-true}
      ORCHESTRATOR_RECOVER_IDLE_MS: ${ORCHESTRATOR_RECOVER_IDLE_MS:-60000}
      JOB_RECOVERY_ENABLED: ${JOB_RECOVERY_ENABLED:-true}
      DEV_RESUME_RENDER_ENABLED: ${DEV_RESUME_RENDER_ENABLED:-false}
    ports:
      - "18000:8000"
    depends_on:
      - db
      - redis
  planner:
    build:
      context: .
      dockerfile: services/planner/Dockerfile
    environment:
      REDIS_URL: redis://redis:6379/0
      PLANNER_MODE: llm
      LLM_PROVIDER: openai
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_MODEL: ${OPENAI_MODEL}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com}
      OPENAI_TEMPERATURE: ${OPENAI_TEMPERATURE}
      OPENAI_MAX_OUTPUT_TOKENS: ${OPENAI_MAX_OUTPUT_TOKENS}
      OPENAI_TIMEOUT_S: ${OPENAI_TIMEOUT_S}
      OPENAI_MAX_RETRIES: ${OPENAI_MAX_RETRIES}
    depends_on:
      - redis
  worker:
    build:
      context: .
      dockerfile: services/worker/Dockerfile
    environment:
      REDIS_URL: redis://redis:6379/0
      LOG_CANDIDATE_RESUME: "true"
      LOG_CANDIDATE_RESUME_FULL: "true"
      # LOG_CANDIDATE_RESUME_PREVIEW_CHARS: "200"
      TOOL_HTTP_FETCH_ENABLED: "false"
      TOOL_HTTP_FETCH_ALLOWLIST: ${TOOL_HTTP_FETCH_ALLOWLIST:-}
      TOOL_OUTPUT_SIZE_CAP: ${TOOL_OUTPUT_SIZE_CAP:-50000}
      WORKER_MODE: llm
      LLM_PROVIDER: openai
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_MODEL: ${OPENAI_MODEL}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com}
      OPENAI_TEMPERATURE: ${OPENAI_TEMPERATURE}
      OPENAI_MAX_OUTPUT_TOKENS: ${OPENAI_MAX_OUTPUT_TOKENS}
      OPENAI_TIMEOUT_S: ${OPENAI_TIMEOUT_S}
      OPENAI_MAX_RETRIES: ${OPENAI_MAX_RETRIES}
      CODING_AGENT_TIMEOUT_S: ${CODING_AGENT_TIMEOUT_S:-30}
      CODER_HTTP_TIMEOUT_S: ${CODER_HTTP_TIMEOUT_S:-30}
      CODER_API_URL: ${CODER_API_URL:-http://coder:8000}
      RESUME_TAILOR_API_URL: ${RESUME_TAILOR_API_URL:-http://tailor:8000}
      WORKSPACE_DIR: /shared/workspace
      GITHUB_TOKEN: ${GITHUB_TOKEN}
    volumes:
      - ./artifacts:/shared/artifacts
      - ./workspace:/shared/workspace
    depends_on:
      - redis
      - coder
      - tailor
  critic:
    build:
      context: .
      dockerfile: services/critic/Dockerfile
    environment:
      REDIS_URL: redis://redis:6379/0
      CRITIC_ENABLED: "false"
      CRITIC_MAX_REWORKS: "2"
    depends_on:
      - redis
  policy:
    build:
      context: .
      dockerfile: services/policy/Dockerfile
    environment:
      REDIS_URL: redis://redis:6379/0
      POLICY_GATE_ENABLED: ${POLICY_GATE_ENABLED:-false}
      POLICY_MODE: dev
      POLICY_CONFIG_PATH: /app/config/policy.yaml
      TOOL_HTTP_FETCH_ENABLED: "false"
    volumes:
      - ./config:/app/config
    depends_on:
      - redis
  tailor:
    build:
      context: .
      dockerfile: services/tailor/Dockerfile
    environment:
      LLM_PROVIDER: ${LLM_PROVIDER:-mock}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_MODEL: ${OPENAI_MODEL}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com}
      OPENAI_TEMPERATURE: ${OPENAI_TEMPERATURE}
      OPENAI_MAX_OUTPUT_TOKENS: ${OPENAI_MAX_OUTPUT_TOKENS}
      OPENAI_TIMEOUT_S: ${OPENAI_TIMEOUT_S}
      OPENAI_MAX_RETRIES: ${OPENAI_MAX_RETRIES}
    ports:
      - "18011:8000"
  coder:
    build:
      context: .
      dockerfile: services/coder/Dockerfile
    environment:
      LLM_PROVIDER: ${LLM_PROVIDER:-mock}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_MODEL: ${OPENAI_MODEL}
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-https://api.openai.com}
      OPENAI_TEMPERATURE: ${OPENAI_TEMPERATURE}
      OPENAI_MAX_OUTPUT_TOKENS: ${OPENAI_MAX_OUTPUT_TOKENS}
      OPENAI_TIMEOUT_S: ${OPENAI_TIMEOUT_S}
      OPENAI_MAX_RETRIES: ${OPENAI_MAX_RETRIES}
    ports:
      - "18010:8000"
  ui:
    build:
      context: .
      dockerfile: services/ui/Dockerfile
      args:
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:18000}
        NEXT_PUBLIC_DEV_TOOLS: ${NEXT_PUBLIC_DEV_TOOLS:-false}
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:18000}
      NEXT_PUBLIC_DEV_TOOLS: ${NEXT_PUBLIC_DEV_TOOLS:-false}
    ports:
      - "3002:3000"
    depends_on:
      - api
  jaeger:
    image: jaegertracing/all-in-one:1.53
    ports:
      - "16686:16686"
      - "14318:4318"
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./docs/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./docs/prometheus.rules.yml:/etc/prometheus/prometheus.rules.yml
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./docs/grafana/provisioning:/etc/grafana/provisioning
      - ./docs/grafana/dashboards:/var/lib/grafana/dashboards

volumes:
  db-data:
  grafana-data:
